# ConfigMap configuration
config:
  enable: true
  file: conf/default-run.yaml

# vLLM Provider configuration
providers:
  vllm:
    llama-4-scout:
      modelId: "vllm-inference/llama-4-scout-17b-16e-w4a16"
      providerModelId: "llama-4-scout-17b-16e-w4a16"  # Actual model name on vLLM server
      url: "https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
      maxTokens: "4096"
      tlsVerify: "false"
      apiTokenSecret: "LLAMA_4_SCOUT_API_TOKEN"
  
  # RAGAS Provider for RAG evaluation metrics
  ragas:
    embeddingModel: "granite-embedding-125m"
    defaultMaxTokens: "2048"
    defaultTemperature: "0.1"

# MCP Server endpoints
mcpServers:
  openshift:
    uri: "http://kubernetes-mcp-kubernetes-mcp-server.llama-stack-example.svc.cluster.local:8080/sse"

# server:
#   distribution:
#     image: registry.redhat.io/rhoai/odh-llama-stack-core-rhel9:v2.25
