# ConfigMap configuration
config:
  enable: true
  file: conf/default-run.yaml

# vLLM Provider configuration
providers:
  vllm:
    llama-4-scout:
      modelId: "llama-4-scout-17b-16e-w4a16"
      url: "https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
      maxTokens: "4096"
      tlsVerify: "false"
      apiTokenSecret: "LLAMA_4_SCOUT_API_TOKEN"

# MCP Server endpoints
mcpServers:
  openshift:
    uri: "http://ocp-mcp-server:8000/sse"
  slack:
    uri: "http://slack-mcp-server:80/sse"

# server:
#   distribution:
#     image: registry.redhat.io/rhoai/odh-llama-stack-core-rhel9:v2.25
