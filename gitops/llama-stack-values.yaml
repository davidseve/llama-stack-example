# ConfigMap configuration
config:
  enable: true
  file: conf/default-run.yaml

# vLLM Provider configuration
providers:
  vllm:
    llama-4-scout:
      modelId: "vllm-inference/llama-4-scout-17b-16e-w4a16"
      providerModelId: "llama-4-scout-17b-16e-w4a16"  # Actual model name on vLLM server
      url: "https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
      maxTokens: "4096"
      tlsVerify: "false"
      apiTokenSecret: "LLAMA_4_SCOUT_API_TOKEN"
  
  # RAGAS Provider for RAG evaluation metrics
  ragas:
    embeddingModel: "sentence-transformers/ibm-granite/granite-embedding-125m-english"
    defaultMaxTokens: "2048"
    defaultTemperature: "0.1"
  postgres:
    enabled: true
    host: "postgres.llama-stack-example.svc.cluster.local"
    port: "5432"
    secretName: "postgres-secret"  # K8s secret containing POSTGRES_DB, POSTGRES_USER, POSTGRES_PASSWORD

   
  # Kubeflow/DSPA configuration for ragas-remote
  # Ref: https://github.com/opendatahub-io/llama-stack-provider-ragas/blob/main/demos/llama-stack-openshift/README.md
  kubeflow:
    enabled: false
    # IMPORTANT: Must be external route - pipeline pods need to reach Llama Stack
    llamaStackUrl: "https://llama-stack-example-llama-stack-example.apps.ocp.sandbox5296.opentlc.com"
    resultsS3Prefix: "ragas-results"
    s3CredentialsSecretName: "minio-credentials"
    # Internal service URL for DSPA (HTTP - podToPodTLS disabled in DSPA)
    pipelinesEndpoint: "http://ds-pipeline-dspa.data-science-project.svc.cluster.local:8888"
    namespace: "data-science-project"
    baseImage: "quay.io/modh/odh-llama-stack-ragas-runner-image:v0.1.1"
    sslVerify: false  # Use false with podToPodTLS: false in DSPA TODO verify if it works or not

# MCP Server endpoints
mcpServers:
  openshift:
    uri: "http://kubernetes-mcp-kubernetes-mcp-server.llama-stack-example.svc.cluster.local:8080/sse"

# server:
#   distribution:
#     image: registry.redhat.io/rhoai/odh-llama-stack-core-rhel9:v2.25
