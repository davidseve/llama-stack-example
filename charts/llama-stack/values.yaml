# Default values for llama-stack.
# This is a YAML-formatted file.
# Declare variables to be substituted into your templates.

# Global configuration
global:
  name: llama-stack-example
  namespace: llama-stack-example
  labels:
    dashboard: 'true'

route:
  enable: true

# Secret configuration
secret:
  name: llama-stack-inference-model-secret
  data:
    INFERENCE_MODEL: ""  # Base64 encoded value
    VLLM_URL: ""         # Base64 encoded value
    VLLM_TLS_VERIFY: ""  # Base64 encoded value
    VLLM_API_TOKEN: ""   # Base64 encoded value


# LlamaStackDistribution configuration
llamastack:
  replicas: 1

# Server configuration
server:
  containerSpec:
    name: llama-stack
    port: 8321
    resources:
      limits:
        cpu: "2"
        memory: 12Gi
      requests:
        cpu: 250m
        memory: 500Mi
    # Environment variables from secrets
    env:
      inferenceModel:
        secretName: llama-stack-inference-model-secret
        key: INFERENCE_MODEL
      vllmUrl:
        secretName: llama-stack-inference-model-secret
        key: VLLM_URL
      vllmTlsVerify:
        secretName: llama-stack-inference-model-secret
        key: VLLM_TLS_VERIFY
      # vllmApiToken:
      #   secretName: llama-stack-inference-model-secret
      #   key: VLLM_API_TOKEN
      milvusDbPath: /tmp/milvus.db
      fmsOrchestratorUrl: http://localhost

  # Distribution configuration
  distribution:
    image: quay.io/opendatahub/llama-stack:odh

  # Storage configuration
  storage:
    size: 5Gi

# Application configuration values
appConfig:
  # Inference configuration
  inference:
    llama3b:
      url: "http://llama3b-service:8000/v1"
      model: "meta-llama/Llama-3.2-3B-Instruct"
      token: "fake"
    
  # Storage configuration
  storage:
    sqliteStoreDir: "~/.llama/distributions/remote-vllm"
    milvusDbPath: "/tmp/milvus.db"
  
  # API Keys (these should typically come from secrets)
  apiKeys:
    openai: ""
    braveSearch: ""
    tavilySearch: ""
  
  # Telemetry configuration
  telemetry:
    serviceName: "llama-stack"
    sinks: "console, sqlite"
    otelTraceEndpoint: ""
    sqliteDbPath: "~/.llama/distributions/remote-vllm/trace_store.db"
  
  # MCP endpoints
  mcp:
    openshift:
      uri: "http://ocp-mcp-server:8000/sse"
    slack:
      uri: "http://slack-mcp-server:80/sse"
