# Secret configuration
secret:
  name: llama-stack-inference-model-secret
  data:
    INFERENCE_MODEL: "llama-3-2-3b"  # Base64 encoded value
    VLLM_URL: "https://llama-3-2-3b-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"         # Base64 encoded value
    VLLM_TLS_VERIFY: "false"  # Base64 encoded value
config:
  enable: true
  # file: conf/default-run.yaml
  file: conf/mcp-run-alvaro.yaml
